{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=None)\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None)\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "# print(type(X), type(Y)) # class 'numpy.ndarray'\n",
    "# X.shape (200000, 14) Y.shape (200000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print the minimum, maximum, mean, and the variance of all of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum of features:  [ 1.9300e+02  1.9000e+02  2.1497e+02  2.0542e+02  1.0000e+01  0.0000e+00\n",
      "  0.0000e+00  0.0000e+00  6.8146e-01  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "  1.0074e+00 -9.9990e+02]\n",
      "maximum of features:  [2.5300e+02 2.5050e+02 2.5250e+02 2.5250e+02 1.7130e+04 1.2338e+04\n",
      " 9.2380e+03 3.5796e+01 1.9899e+01 1.1368e+01 2.1466e+01 1.4745e+01\n",
      " 2.7871e+02 7.8250e+02]\n",
      "mean of features:  [2.41797220e+02 2.28228260e+02 2.41796298e+02 2.33649299e+02\n",
      " 2.86797959e+03 8.84073295e+02 1.73553355e+02 3.04719572e+00\n",
      " 6.35196722e+00 1.92523232e+00 4.29379349e+00 2.80947178e+00\n",
      " 1.03679146e+01 7.87334450e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum of features: \",X.min(axis=0))\n",
    "print(\"maximum of features: \",X.max(axis=0))\n",
    "print(\"mean of features: \",X.mean(axis=0))\n",
    "print(\"variance of features: \",X.var(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "Xt, Yt = Xtr[:5000], Ytr[:5000] # subsample for efficiency (you can go higher)\n",
    "XtS, params = ml.rescale(Xt) # Normalize the features\n",
    "XvS, _ = ml.rescale(Xva, params) # Normalize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the min, maximum, mean, and the variance of the rescaled features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain:\n",
      "minimum of features:  [ -4.64112009  -3.97499512  -4.37422128  -2.88879199  -0.8817656\n",
      "  -0.49365693  -0.20639577  -1.08823103  -2.0635883   -0.91480428\n",
      "  -2.0894829   -1.97542587  -0.66938615 -27.92233557]\n",
      "maximum of features:  [ 1.24664952  1.63558455  1.74893152  1.91322229  4.42913327  6.51424569\n",
      " 10.70075939  8.59776734  4.15412257  4.45336179  6.91614071  5.60015478\n",
      " 18.61104599 14.57421925]\n",
      "mean of features:  [-2.18635110e-15  1.48525636e-15  1.10162681e-14 -1.04740938e-15\n",
      "  3.04201109e-18 -6.66022792e-17 -3.06255021e-16  1.52030055e-15\n",
      "  5.02918818e-15 -3.59379193e-16 -4.05218081e-15  7.26210203e-15\n",
      " -2.82971424e-15 -2.60907962e-16]\n",
      "variance of features:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Xval:\n",
      "minimum of features:  [ 1.9300e+02  1.9000e+02  2.1497e+02  2.0542e+02  1.0000e+01  0.0000e+00\n",
      "  0.0000e+00  0.0000e+00  6.8146e-01  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "  1.0271e+00 -9.9990e+02]\n",
      "maximum of features:  [2.5300e+02 2.5050e+02 2.5250e+02 2.5250e+02 1.7130e+04 1.2338e+04\n",
      " 9.2380e+03 3.5796e+01 1.8107e+01 1.1368e+01 1.8918e+01 1.4745e+01\n",
      " 2.7871e+02 7.2300e+02]\n",
      "mean of features:  [2.41773483e+02 2.28213467e+02 2.41775832e+02 2.33652770e+02\n",
      " 2.87541088e+03 8.92593250e+02 1.78037250e+02 3.03930478e+00\n",
      " 6.35889920e+00 1.92969635e+00 4.28651362e+00 2.81202073e+00\n",
      " 1.03684060e+01 7.60302000e+00]\n",
      "variance of features:  [8.29838937e+01 9.17214057e+01 3.61216837e+01 9.54352598e+01\n",
      " 1.06775148e+07 3.29249832e+06 7.65983909e+05 7.32033825e+00\n",
      " 6.37171223e+00 4.33519011e+00 4.01915569e+00 1.98135048e+00\n",
      " 1.77860610e+02 1.49734259e+03]\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain:\")\n",
    "print(\"minimum of features: \",XtS.min(axis=0))\n",
    "print(\"maximum of features: \",XtS.max(axis=0))\n",
    "print(\"mean of features: \",XtS.mean(axis=0))\n",
    "print(\"variance of features: \",XtS.var(axis=0))\n",
    "print(\"Xval:\")\n",
    "print(\"minimum of features: \",Xva.min(axis=0))\n",
    "print(\"maximum of features: \",Xva.max(axis=0))\n",
    "print(\"mean of features: \",Xva.mean(axis=0))\n",
    "print(\"variance of features: \",Xva.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# download and create datasets\n",
    "train_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=True, \n",
    "                               transform=transforms,\n",
    "                               download=True)\n",
    "\n",
    "valid_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=False, \n",
    "                               transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0]... cmap='gray')\n",
    "plt.text(10, -2, 'The label is ' + str(....))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 15\n",
    "\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset=..., \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=...)\n",
    "\n",
    "valid_loader = DataLoader(dataset=..., \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    '''\n",
    "    Train one epoch.\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_loader:\n",
    "\n",
    "        optimizer.....\n",
    "\n",
    "    \n",
    "        # Forward pass\n",
    "        y_hat, _ = model(X) \n",
    "        loss = ....\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Backward pass\n",
    "        .....\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, criterion):\n",
    "    '''\n",
    "    Function for the validation step of the training loop.\n",
    "    Returns the model and the loss on the test set.\n",
    "    '''\n",
    "   \n",
    "    model......\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "\n",
    "        # Forward pass and record loss\n",
    "        ...\n",
    "        \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "        \n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validate(valid_loader, model, criterion)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy(model, train_loader,)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_acc = get_accuracy(model, valid_loader)\n",
    "            valid_accs.append(valid_acc)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "    \n",
    "    performance = {\n",
    "        'train_losses':train_losses,\n",
    "        'valid_losses': valid_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'valid_acc':valid_accs\n",
    "    }\n",
    "    \n",
    "    return model, optimizer, performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.....\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            ....\n",
    "            ....\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "\n",
    "    \n",
    "def plot_performance(performance):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (16, 4.5))\n",
    "    for key, value in performance.items():\n",
    "        if 'loss' in key:\n",
    "            ax[0].plot(value, label=key) \n",
    "        else:\n",
    "            ax[1].plot(value, label=key) \n",
    "    ax[0].set(title=\"Loss  over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss')\n",
    "    ax[1].set(title=\"accuracy over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss')\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "    ....\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ......\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        logits = ...\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = LeNet5(N_CLASSES)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, performance_1 = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
    "plot_performance(performance_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "layers = [1024, 256, 64, 16, N_CLASSES]\n",
    "model = MLP(layers)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, performance_2 = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
